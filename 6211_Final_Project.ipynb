{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chloepar/Stack_Overflow_Survey_Exploration_DSBA6211/blob/main/6211_Final_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import requests, zipfile, io, time, os, logging"
      ],
      "metadata": {
        "id": "QbvNTAC5WRM-"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "headers = {\n",
        "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0 Safari/537.36'\n",
        "}"
      ],
      "metadata": {
        "id": "Uv_r4zASoByb"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load surveys for 2011 - 2015, 2017 - 2025 (2016 has a different folder format)"
      ],
      "metadata": {
        "id": "EVT3oLauzGE_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def find_csv_in_dir(directory):\n",
        "    for root, dirs, files in os.walk(directory):\n",
        "        for file in files:\n",
        "            if file.endswith('.csv'):\n",
        "                return os.path.join(root, file)\n",
        "    raise FileNotFoundError(f\"No CSV found in {directory}\")\n",
        "\n",
        "# Define available years\n",
        "available_years = range(2011, 2026)\n",
        "dfs = {}\n",
        "\n",
        "# Retry settings\n",
        "max_retries = 10\n",
        "base_delay = 30  # seconds\n",
        "\n",
        "for year in available_years:\n",
        "    url = f'https://survey.stackoverflow.co/datasets/stack-overflow-developer-survey-{year}.zip'\n",
        "    extract_path = f'/content/survey_{year}'\n",
        "    print(f\"\\nüì¶ Processing {year}...\")\n",
        "\n",
        "    for attempt in range(1, max_retries + 1):\n",
        "        try:\n",
        "            r = requests.get(url)\n",
        "            content_type = r.headers.get('Content-Type', '')\n",
        "\n",
        "            if r.status_code == 429:\n",
        "                retry_after = int(r.headers.get('Retry-After', base_delay))\n",
        "                print(f\"‚è≥ Rate limited. Waiting {retry_after} seconds before retrying...\")\n",
        "                time.sleep(retry_after)\n",
        "                continue\n",
        "\n",
        "            if r.status_code == 200 and 'zip' in content_type:\n",
        "                z = zipfile.ZipFile(io.BytesIO(r.content))\n",
        "                z.extractall(extract_path)\n",
        "\n",
        "                csv_path = find_csv_in_dir(extract_path)\n",
        "                df = pd.read_csv(csv_path, encoding='ISO-8859-1')\n",
        "                df['year'] = year  # Add the year column\n",
        "                dfs[year] = df\n",
        "                print(f\"‚úÖ Loaded {year} survey with {len(df)} rows.\")\n",
        "                break  # success, exit retry loop\n",
        "            else:\n",
        "                print(f\"‚ùå Failed for {year}: Status {r.status_code}, Content-Type {content_type}\")\n",
        "                break  # don't retry non-429 failures\n",
        "\n",
        "        except zipfile.BadZipFile:\n",
        "            print(f\"‚ö†Ô∏è Invalid ZIP file for {year}. Attempt {attempt}/{max_retries}\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Error on attempt {attempt}/{max_retries} for {year}: {e}\")\n",
        "\n",
        "        # Exponential backoff before next retry\n",
        "        wait = base_delay * (2 ** (attempt - 1))\n",
        "        print(f\"üîÅ Retrying in {wait} seconds...\")\n",
        "        time.sleep(wait)\n"
      ],
      "metadata": {
        "id": "SciI-xlaqGjt",
        "outputId": "eec76d87-5261-475a-9501-5a084466c56b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üì¶ Processing 2011...\n",
            "‚úÖ Loaded 2011 survey with 2814 rows.\n",
            "\n",
            "üì¶ Processing 2012...\n",
            "‚úÖ Loaded 2012 survey with 6244 rows.\n",
            "\n",
            "üì¶ Processing 2013...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2658173456.py:37: DtypeWarning: Columns (46,48,62) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(csv_path, encoding='ISO-8859-1')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Loaded 2013 survey with 9743 rows.\n",
            "\n",
            "üì¶ Processing 2014...\n",
            "‚úÖ Loaded 2014 survey with 7644 rows.\n",
            "\n",
            "üì¶ Processing 2015...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2658173456.py:37: DtypeWarning: Columns (5,108,121,196,197,198) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(csv_path, encoding='ISO-8859-1')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Loaded 2015 survey with 26087 rows.\n",
            "\n",
            "üì¶ Processing 2016...\n",
            "‚úÖ Loaded 2016 survey with 0 rows.\n",
            "\n",
            "üì¶ Processing 2017...\n",
            "‚úÖ Loaded 2017 survey with 51392 rows.\n",
            "\n",
            "üì¶ Processing 2018...\n",
            "‚è≥ Rate limited. Waiting 30 seconds before retrying...\n",
            "‚è≥ Rate limited. Waiting 30 seconds before retrying...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2658173456.py:37: DtypeWarning: Columns (8,12,13,14,15,16,50,51,52,53,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(csv_path, encoding='ISO-8859-1')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Loaded 2018 survey with 98855 rows.\n",
            "\n",
            "üì¶ Processing 2019...\n",
            "‚úÖ Loaded 2019 survey with 88883 rows.\n",
            "\n",
            "üì¶ Processing 2020...\n",
            "‚úÖ Loaded 2020 survey with 64461 rows.\n",
            "\n",
            "üì¶ Processing 2021...\n",
            "‚è≥ Rate limited. Waiting 30 seconds before retrying...\n",
            "‚è≥ Rate limited. Waiting 30 seconds before retrying...\n",
            "‚úÖ Loaded 2021 survey with 83439 rows.\n",
            "\n",
            "üì¶ Processing 2022...\n",
            "‚úÖ Loaded 2022 survey with 73268 rows.\n",
            "\n",
            "üì¶ Processing 2023...\n",
            "‚è≥ Rate limited. Waiting 30 seconds before retrying...\n",
            "‚è≥ Rate limited. Waiting 30 seconds before retrying...\n",
            "‚úÖ Loaded 2023 survey with 89184 rows.\n",
            "\n",
            "üì¶ Processing 2024...\n",
            "‚úÖ Loaded 2024 survey with 65437 rows.\n",
            "\n",
            "üì¶ Processing 2025...\n",
            "‚è≥ Rate limited. Waiting 30 seconds before retrying...\n",
            "‚è≥ Rate limited. Waiting 30 seconds before retrying...\n",
            "‚úÖ Loaded 2025 survey with 49123 rows.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extract 2016"
      ],
      "metadata": {
        "id": "kAgiAe-xzTMJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "def download_and_extract_zip(url, headers, extract_path, max_retries=3, delay=5):\n",
        "    for attempt in range(1, max_retries + 1):\n",
        "        try:\n",
        "            logging.info(f\"Attempt {attempt}: Downloading {url}\")\n",
        "            response = requests.get(url, headers=headers, timeout=30)\n",
        "            response.raise_for_status()  # Raise HTTPError for bad responses\n",
        "            with zipfile.ZipFile(io.BytesIO(response.content)) as z:\n",
        "                z.extractall(extract_path)\n",
        "            logging.info(\"Download and extraction successful.\")\n",
        "            return True\n",
        "        except (requests.RequestException, zipfile.BadZipFile) as e:\n",
        "            logging.warning(f\"Attempt {attempt} failed: {e}\")\n",
        "            if attempt < max_retries:\n",
        "                time.sleep(delay)\n",
        "            else:\n",
        "                logging.error(\"All attempts failed.\")\n",
        "                return False\n",
        "\n",
        "# Parameters\n",
        "url_2016 = 'https://survey.stackoverflow.co/datasets/stack-overflow-developer-survey-2016.zip'\n",
        "extract_path_2016 = '/content/survey_2016'\n",
        "\n",
        "# Run download\n",
        "success = download_and_extract_zip(url_2016, headers, extract_path_2016)\n",
        "\n",
        "# Load CSV if successful\n",
        "if success:\n",
        "    csv_path = '/content/survey_2016/2016 Stack Overflow Survey Results/2016 Stack Overflow Survey Responses.csv'\n",
        "    try:\n",
        "        df_2016 = pd.read_csv(csv_path, encoding='ISO-8859-1')\n",
        "        df_2016['year'] = '2016'  # Add the year column\n",
        "        logging.info(\"CSV loaded successfully.\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Failed to load CSV: {e}\")\n",
        "else:\n",
        "    df_2016 = None\n"
      ],
      "metadata": {
        "id": "rHf2-vT0y4YH"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extract dataframes for each year"
      ],
      "metadata": {
        "id": "A9SZBbgpzQhR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for year, df in dfs.items():\n",
        "    globals()[f'df_{year}'] = df"
      ],
      "metadata": {
        "id": "MJF04VQPtQyd"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Combine all dataframes from 2011 to 2025\n"
      ],
      "metadata": {
        "id": "_mVm576H0G1O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_dfs = list(dfs.values()) + [df_2016]\n",
        "df = pd.concat(all_dfs, ignore_index=True)\n",
        "display(df.head())"
      ],
      "metadata": {
        "id": "9-dXUht8bKQA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_dfs.describe(include='all').T"
      ],
      "metadata": {
        "id": "bwek7jSZ45-A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify columns in each DataFrame for all available years\n",
        "all_years = list(dfs.keys()) + [2016] # Include 2016 in the list of years\n",
        "cols_by_year = {year: set(dfs[year].columns) if year != 2016 else set(df_2016.columns) for year in all_years}\n",
        "\n",
        "# Get all unique columns across all years\n",
        "all_unique_cols = set().union(*cols_by_year.values())\n",
        "\n",
        "# Create a dictionary to store column presence in each year\n",
        "column_presence = {year: [col in cols_by_year[year] for col in all_unique_cols] for year in all_years}\n",
        "\n",
        "# Create a pandas DataFrame (matrix) from the dictionary\n",
        "column_matrix = pd.DataFrame(column_presence, index=list(all_unique_cols))\n",
        "\n",
        "# Add a column to count the number of dataframes the column is in\n",
        "column_matrix['PresenceCount'] = column_matrix.sum(axis=1)\n",
        "\n",
        "# Sort the matrix by 'PresenceCount' in descending order\n",
        "column_matrix_sorted = column_matrix.sort_values(by='PresenceCount', ascending=False)\n",
        "\n",
        "# Drop the 'PresenceCount' column for the final display\n",
        "column_matrix_sorted = column_matrix_sorted.drop(columns=['PresenceCount'])\n",
        "\n",
        "# Display the sorted matrix\n",
        "display(column_matrix_sorted)"
      ],
      "metadata": {
        "id": "i8qD4KbA48ji"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for missing values in 'Country' column\n",
        "print(\"Missing values in 'Country' column:\")\n",
        "print(df['Country'].isnull().sum())\n",
        "\n",
        "# Drop rows with missing 'Country' values\n",
        "df_country = df.dropna(subset=['Country']).copy()\n",
        "\n",
        "# Group by 'Country' and 'Year' and count occurrences\n",
        "country_counts = df_country.groupby(['Country', 'Year']).size().reset_index(name='Count')\n",
        "\n",
        "# Display the counts\n",
        "display(country_counts.head())\n",
        "\n",
        "# Pivot the table for easier plotting\n",
        "country_pivot = country_counts.pivot(index='Country', columns='Year', values='Count').fillna(0)\n",
        "\n",
        "# Display the pivoted table\n",
        "display(country_pivot.head())\n",
        "\n",
        "# Optional: Visualize the top N countries over the years\n",
        "top_n = 10\n",
        "top_countries = country_pivot.sum(axis=1).nlargest(top_n).index\n",
        "country_pivot_top = country_pivot.loc[top_countries]\n",
        "\n",
        "# Reset index to make 'Country' a column and melt for plotting\n",
        "country_pivot_top_reset = country_pivot_top.reset_index()\n",
        "country_melted = country_pivot_top_reset.melt(id_vars='Country', var_name='Year', value_name='Count')\n",
        "\n",
        "plt.figure(figsize=(15, 7))\n",
        "sns.lineplot(data=country_melted, x='Year', y='Count', hue='Country')\n",
        "\n",
        "plt.title(f'Top {top_n} Stack Overflow Users by Country and Year')\n",
        "plt.xlabel('Year')\n",
        "plt.ylabel('Number of Users')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "VSrwEVNCWj5_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}